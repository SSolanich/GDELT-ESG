{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import spacy\n",
    "from tika import parser\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import stanza\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos una lista de tripletas que contienen el indice de la empresa, su nombre y la dirección a su reporte en pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esg_urls_rows = [\n",
    "  ['aguas andinas', 'pdf_es/reporte-integrado-aguas-andinas-2019-v1.pdf'],\n",
    "  ['cap', 'pdf_es/memoria_cap_s_a__2019.pdf'],\n",
    "  ['ecl', 'pdf_es/201704-Reporte-Integrado-EECL-compressed.pdf'],\n",
    "  ['endesa-cl', 'pdf_es/Memoria-Enel-Chile-2019.pdf'],\n",
    "  ['aesgener', 'pdf_es/Memoria-2019-ESP-AES-Gener.pdf'],\n",
    "  ['sqm', 'pdf_es/Reporte-2019-SQM-ESP.pdf'],\n",
    "  ['colbun', 'pdf_es/MEMORIA-COLBUN-2019.pdf'],\n",
    "  ['cmpc', 'pdf_es/Reporte_2019.pdf'],\n",
    "  ['ccu', 'pdf_es/CCU 2019 - 30 marzov6 EEFF completos.pdf'],\n",
    "  ['cencosud', 'pdf_es/Memoria-Anual-Integrada-Cencosud-14042020v2.pdf'],\n",
    "  ['concha toro', 'pdf_es/Vina_Concha_y_Toro_Memoria_2019-1-1.pdf'],\n",
    "  ['itaucorp', 'pdf_es/200504+Memoria_Integrada_2019.pdf'],\n",
    "  ['entel', 'pdf_es/Reporte_Sustentabilidad_Entel_2019.pdf'],\n",
    "  ['falabella', 'pdf_es/2019-Memoria-falabella.pdf'],\n",
    "  ['ilc', 'pdf_es/Memoria-Anual-ILC-2019_Razonado_Fecu.pdf'],\n",
    "  ['latam', 'pdf_es/LATAM-MemoriaIntegrada2019.pdf'],\n",
    "  ['parauco', 'pdf_es/Parque-Arauco-Memoria-Integrada-2019-1-1.pdf'],\n",
    "  ['ripley', 'pdf_es/Memoria-Anual-Ripley-Chile-2019.pdf'],\n",
    "  ['salfacorp', 'pdf_es/Memoria_SalfaCorp_2019_m.pdf'],\n",
    "  ['grupo security', 'pdf_es/gs_memoria_2019.pdf'],\n",
    "  ['sonda', 'pdf_es/Reporte_integrado.pdf'],\n",
    "  ['andina embotelladora', 'pdf_es/Memoria Anual integrada 2019.pdf'],\n",
    "  ['bci', 'pdf_es/Memoria_BCI_2019.pdf'],\n",
    "  ['santander', 'pdf_es/memorial_anual_2019.pdf'],\n",
    "  ['endesa am', 'pdf_es/EnelAmericaInforme de Sostenibilidad_2019.pdf']\n",
    "]\n",
    "esg_urls_rows = [[i, name, link] for i, [name,link] in enumerate(esg_urls_rows)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esg_urls_pd = pd.DataFrame(esg_urls_rows, columns=['idx', 'company', 'url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como los reportes son integrados y no tratan tematicas de sustentabilidad en su totalidad, definimos una secuencia de paginas relevantes a extraer para cada reporte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nseq(start, finish, offset=0):\n",
    "        return list(range(start+offset, finish+offset+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [nseq(18, 26) + nseq(33, 68) + nseq(74, 94) + nseq(116, 162), #andina\n",
    "         nseq(69, 81, 2) + nseq(91, 92, 2),   #cap\n",
    "         [32] + nseq(36,37) + nseq(44,45) + nseq(61,105) + nseq(108, 119),    #engie\n",
    "         nseq(85, 184),    #enel\n",
    "         nseq(18, 59) + nseq(124,277) + nseq(284,291) + nseq(348,349),  #aesgener\n",
    "         nseq(1,189),    #sqm\n",
    "         nseq(70,105) + nseq(116,135) + nseq(144,267),   #colbun\n",
    "         nseq(17,18) + nseq(21, 24) + nseq(33,63) + nseq(68,70) + nseq(74, 88),   #cmpc\n",
    "         [24, 29, 36, 39, 45, 51] + nseq(63, 67),   #ccu\n",
    "         nseq(19,139),   #cencosud\n",
    "         nseq(17,32) + nseq(54,103),   #concha y toro\n",
    "         nseq(20,153) + nseq(162,165),   #itau\n",
    "         nseq(5,105),   #entel\n",
    "         nseq(7,12) + nseq(17,70) + nseq(133,141),   #falabella\n",
    "         nseq(27,30) + nseq(31,32) + nseq(42,45),   #ilc\n",
    "         [14, 27,28] + nseq(39,54) + [61] + nseq(63,116),   #latam\n",
    "         nseq(4,52) + nseq(80,105),   #parque arauco\n",
    "         nseq(49,62),   #ripley\n",
    "         nseq(16,23) + nseq(93,114) + nseq(119,144) + nseq(149,151) + nseq(154,155) + nseq(157,173),   #salfacorp\n",
    "         nseq(26, 39) + nseq(78,101),   #security\n",
    "         nseq(33,84) + nseq(91,98) + nseq(148,150),   #sonda\n",
    "         nseq(1,92),   #koandina\n",
    "         nseq(32,88),   #bci\n",
    "         nseq(19,136) + nseq(157,167) + nseq(173,182),   #santander\n",
    "         nseq(1,262)   #enel americas\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos el listados de paginas junto con el indices de cada empresa para extraer las paginas que correspondan según el caso. ```extract_content_tika``` utiliza tika para extraer todo el texto encontrado en el reporte pdf.\n",
    "Se parsea en formato xhtml para facilitar la extracción de secciones de texto y delimitar entre páginas. El paquete BeautifulSoup ayuda a procesar este formato.\n",
    "\n",
    "Se debe considerar que esta función se aplica en estilo .apply de pandas, es decir, se aplica a cada fila del dataframe, en particular ```esg_urls_pd```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_tika(row, pages):\n",
    "    file_data = []\n",
    "    _buffer = io.StringIO()\n",
    "    data = parser.from_file(row.url, xmlContent=True)#parser.from_file(\"drive/MyDrive/local/\" + row.url, xmlContent=True)\n",
    "    xhtml_data = BeautifulSoup(data['content'])\n",
    "    for page, content in enumerate(xhtml_data.find_all('div', attrs={'class': 'page'})):\n",
    "        if page + 1 in pages[row.idx]:\n",
    "            _buffer.write(str(content))\n",
    "            parsed_content = parser.from_buffer(_buffer.getvalue())\n",
    "            _buffer.truncate()\n",
    "            file_data.append(parsed_content['content'])\n",
    "    return \"\\n\".join(file_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez extraido el texto se procede a su limpieza, en el orden que muestra el código se realiza:\n",
    "\n",
    "- Se eliminan todos los saltos de línea, se reemplazan por espacios.\n",
    "- Dado que cuando una palabra no alcanza a terminar en la pagina se separa por guíon, se revierte juntando todas las palabras que están separadas por guión.\n",
    "- Se eliminan todos los enlaces e hipervínculos web.\n",
    "- Se eliminan todos los caracteres especiales no alfanúmericos excluyendo el punto (.) pues este señala el inicio y fin de una oración.\n",
    "- Se eliminan números y guiones.\n",
    "- Dadas las transformaciones anteriores pueden quedar multiples espacios juntos. Se reduce la concatenación de espacios a uno solo.\n",
    "- Por la misma razón anterior pueden quedar símbolos de puntuación juntos. Se reducen a uno solo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_content(content):\n",
    "    content = content.str.replace('\\n', ' ', regex=False)\n",
    "    content = content.str.replace('([a-zA-Z0-9]+)\\-(?: *)([a-zA-Z0-9]+)', r'\\1\\2', regex=True) # juntar palabras separadas por guion\n",
    "    content = content.str.replace('(http|https)://[^ ]+', '', regex=True) # eliminar enlaces\n",
    "    content = content.str.replace('[^\\w  \\.]', '', regex=True) # eliminar caracteres especiales\n",
    "    content = content.str.replace('[0-9_]', '', regex=True) # eliminar caracteres especiales\n",
    "    content = content.str.replace('\\s+', ' ', regex=True) # juntar muchos espacios en uno\n",
    "    content = content.str.replace('[ \\.]{2,}', '.', regex=True) # juntar puntuacion\n",
    "    return content.str.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utiliza el diccionario estandar de nltk para considerar las stopwords en español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos el modelo NLP de stanza para realizar lematización. Se cargan solo los módulos de POS, tokenize y lemma ya que solo utilizaremos esos, con el fin de ahorrar memoria y tiempo de cómputo. Finalmente optamos por el uso de gpu ya que esto acelera el procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_gpu = stanza.Pipeline('es', processors='pos, tokenize, lemma', use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define la función de lemmatización de tal forma que recibe una fila del dataframe (de la columna que contiene el texto) y el modelo de stanza. El modelo es capaz de encontrar oraciones en el texto entregado, pero como el texto entregado es una oración se extrae de inmediato la de indice 0. Luego se hace uso del atributo lemma en cada palabra para concatenarlas en un string y retornarlo, en este punto se puede filtrar por POS si se desea, utilizando el atributo .upos de la palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(row, nlp):\n",
    "    doc = nlp(row)\n",
    "    sentence = doc.sentences[0]\n",
    "    return \" \".join([word.lemma for word in sentence.words if (word.lemma not in stop_words and word.upos != \"VERB\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente se aplica cada paso anteriormente descrito para cada una de las empresas en el dataframe esg_urls_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(esg_urls_pd)):\n",
    "    esg_data = esg_urls_pd[[\"idx\", \"company\", \"url\"]].iloc[[i]]\n",
    "    esg_data[\"content\"] = esg_data.apply(extract_content_tika, args=[pages], axis=1)\n",
    "    esg_data[\"sentences\"] = clean_content(esg_data[\"content\"])\n",
    "    \n",
    "    esg_data = esg_data.drop(columns=[\"idx\", \"url\", \"content\"])\n",
    "    esg_data = esg_data.explode(\"sentences\")\n",
    "    esg_data = esg_data.drop_duplicates()\n",
    "    esg_data = esg_data[esg_data.sentences.str.len() > 5]\n",
    "    esg_data[\"lemma\"] = esg_data.sentences.str.lower()\n",
    "    \n",
    "    esg_data[\"lemma\"] = [lemmatize(x, nlp_gpu) for x in esg_data[\"lemma\"].values]\n",
    "    esg_data = esg_data.reset_index(drop=True)\n",
    "    \n",
    "    filename = esg_data[\"company\"].iloc[0].replace(\" \", \"_\") + \"_noverb\"\n",
    "    esg_data.to_feather(f\"clean_data/{filename}.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se genera un archivo de formato FEATHER por cada empresa, el peso no supera 1 MB en la mayoría de los archivos. La lectura de estos archivos por pandas es simple y rápida."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
