{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing large amounts of text data can prove to be quite challenging, which is why finding ways to optimize each step of pre processing is crucial for a real-life NLP project.\n",
    "\n",
    "In this article I show many different problems you may encounter while cleaning your text, in particular for the purpose of topic modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PDF text extraction\n",
    "\n",
    "This may seem niche for most cases, but I'll include it since it could be of use to more than one reader:\n",
    "\n",
    "When dealing with the extraction of text from pdf files, you may be tempted to use the package pyPDF2, since there's plenty of recommendations for it on most google results for the query \"how to extract text from pdf\". However the results this package gives are quite sub optimal, missing large amounts of text. I instead advise for the package tika, whose parser while not being perfect, increased the amount of properly extracted text by a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import PyPDF2\n",
    "\n",
    "import io\n",
    "from tika import parser\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"pdf_es/CCU 2019 - 30 marzov6 EEFF completos.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content(path_or_url):\n",
    "    \"\"\"\n",
    "    A simple user define function that, given a url, download PDF text content\n",
    "    Parse PDF and return plain text version\n",
    "    \"\"\"\n",
    "    # retrieve PDF binary stream\n",
    "    pdf = PyPDF2.PdfFileReader(path_or_url)  \n",
    "    # access pdf content\n",
    "    text = [pdf.getPage(i).extractText() for i in range(0, pdf.getNumPages())]\n",
    "    # return concatenated content\n",
    "    return \"\\n\".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_tika(path_or_url):\n",
    "    file_data = []\n",
    "    _buffer = io.StringIO()\n",
    "    data = parser.from_file(path_or_url, xmlContent=True)\n",
    "    xhtml_data = BeautifulSoup(data['content'])\n",
    "    for page, content in enumerate(xhtml_data.find_all('div', attrs={'class': 'page'})):\n",
    "        _buffer.write(str(content))\n",
    "        parsed_content = parser.from_buffer(_buffer.getvalue())\n",
    "        _buffer.truncate()\n",
    "        file_data.append(parsed_content['content'])\n",
    "    return \"\\n\".join(file_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to extract text using pyPDF2: 45\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "text_pdf2 = extract_content(pdf_file)\n",
    "delta_time = int(time() - start_time)\n",
    "print(f\"Time taken to extract text using pyPDF2: {delta_time}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-21 15:47:58,476 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to extract text using tika: 77\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "text_tika = extract_content_tika(pdf_file)\n",
    "delta_time = int(time() - start_time)\n",
    "print(f\"Time taken to extract text using tika: {delta_time}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text extracted using pyPDF2: 1748569 characters\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of text extracted using pyPDF2: {len(text_pdf2)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text extracted using tika: 492704776 characters\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of text extracted using tika: {len(text_tika)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a massive increase of 281x times as many characters! Meaning we were successful in extracting much more text data using tika rather than pyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning text\n",
    "\n",
    "From this point on the main discussion will be why avoiding pandas .apply() method is a bad idea, and how to replace it with much more efficient methods. We'll be using tika's extracted text since that's what would be expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.DataFrame({\"ext\": [\"tika\", \"pypdf2\"], \"text\": [text_tika, text_pdf2]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially what many people would think is \"let's process these strings of text in a per row basis, that way my funcion will apply to each row like a map funcion on a list\", since that's what feels most intuitive to most of us. So they write a per-row-apply-style function for their pandas data and end up with something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_content_row(text):\n",
    "    # remove non ASCII characters\n",
    "    printable = set(string.printable).union(['á','é','í','ó','ú','Á','É','Í','Ó','Ú','ñ','Ñ'])\n",
    "    text = ''.join(filter(lambda x: x in printable, text))\n",
    "    lines = []\n",
    "    prev = \"\"\n",
    "    for line in text.split('\\n'):\n",
    "    # aggregate consecutive lines where text may be broken down\n",
    "    # only if next line starts with a space or previous does not end with dot.\n",
    "        if (line.startswith(' ') or not prev.endswith('.')):\n",
    "            prev = prev + ' ' + line\n",
    "        else:\n",
    "            # new paragraph\n",
    "            lines.append(prev)\n",
    "            prev = line\n",
    "    # don't forget left-over paragraph\n",
    "    lines.append(prev)\n",
    "\n",
    "    # clean paragraphs from extra space, unwanted characters, urls, etc.\n",
    "    # best effort clean up, consider a more versatile cleaner\n",
    "    sentences = []\n",
    "    for line in lines:\n",
    "        # removing header number\n",
    "        line = re.sub(r'^\\s?\\d+(.*)$', r'\\1', line)\n",
    "        # removing trailing spaces\n",
    "        line = line.strip()\n",
    "        # words may be split between lines, ensure we link them back together\n",
    "        line = re.sub('\\s?-\\s?', '-', line)\n",
    "        # remove space prior to punctuation\n",
    "        line = re.sub(r'\\s?([,:;\\.])', r'\\1', line)\n",
    "        # ESG contains a lot of figures that are not relevant to grammatical structure\n",
    "        line = re.sub(r'\\d{5,}', r' ', line)\n",
    "        # remove mentions of URLs\n",
    "        line = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', r' ', line)\n",
    "        # remove multiple spaces\n",
    "        line = re.sub('\\s+', ' ', line)\n",
    "        sentences.append(line)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to process text using pyPDF2: 429s.\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "statements_apply = text_df[text_df.ext==\"tika\"][\"text\"].apply(extract_statements)\n",
    "delta_time = int(time() - start_time)\n",
    "print(f\"Time taken to process text using apply: {delta_time}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I propose a different way of processing text, using what's called a vectorized function.\n",
    "\n",
    "Pandas provides a nice set of vectorized string functions, which act on a column of data evenly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_content_vectorized(text):\n",
    "    text = text.str.replace('\\n', ' ', regex=False)\n",
    "    text = text.str.replace('([a-zA-Z0-9]+)\\-(?: *)([a-zA-Z0-9]+)', r'\\1\\2', regex=True) # juntar palabras separadas por guion\n",
    "    text = text.str.replace('(http|https)://[^ ]+', '', regex=True) # eliminar enlaces\n",
    "    text = text.str.replace('[^\\w  \\.]', '', regex=True) # eliminar caracteres especiales\n",
    "    text = text.str.replace('[0-9_]', '', regex=True) # eliminar caracteres especiales\n",
    "    text = text.str.replace('\\s+', ' ', regex=True) # juntar muchos espacios en uno\n",
    "    text = text.str.replace('[ \\.]{2,}', '.', regex=True) # juntar puntuacion\n",
    "    return text.str.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to process text using vectorized functions: 115s.\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "statements_vectorized = clean_content_vectorized(text_df[text_df.ext==\"tika\"][\"text\"])\n",
    "delta_time = int(time() - start_time)\n",
    "print(f\"Time taken to process text using vectorized functions: {delta_time}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a 4 fold increase in speed changing from the apply method to using vectorized functions. Note that this may vastly improve if your text cleaning process is simpler (since much of what we do in these examples correspond to cleaning characters that arise from the pdf extracting process)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lemmatization\n",
    "\n",
    "Spacy seems to be the go to package for NLP real life application projects, since it's very convenient out of the box and quite perfoming. However when the text is not in english the results of the model are not what we would expect, so a better way to perform lemmatization on text is needed.\n",
    "\n",
    "On the following section we present comparisons between the results from using Spacy's model for lemmatization and a new package that we found, Stanza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import stanza\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize(df, func, cores):\n",
    "    num_of_processes = cores\n",
    "    data_split = np.array_split(df, num_of_processes)\n",
    "    pool = Pool(num_of_processes)\n",
    "    data = pd.concat(pool.map(func, data_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return data\n",
    "\n",
    "def run_on_subset(func, extra_data, data_subset):\n",
    "    # data_subset is a series\n",
    "    return data_subset.apply(func, args=(extra_data))\n",
    "\n",
    "def multiprocess_apply(df, func, cores=8, data=None):\n",
    "    return parallelize(df, partial(run_on_subset, func, data), cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_spacy(row, nlp):\n",
    "    doc = nlp(row)\n",
    "    sentence = doc.sentences\n",
    "    return \" \".join([word.lemma for word in sentence.words if word.lemma not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stanza(row, nlp):\n",
    "    doc = nlp(row)\n",
    "    sentence = doc.sentences[0]\n",
    "    return \" \".join([word.lemma for word in sentence.words if word.lemma not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-21 18:03:49 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| pos       | ancora  |\n",
      "| lemma     | ancora  |\n",
      "=======================\n",
      "\n",
      "2020-12-21 18:03:49 INFO: Use device: gpu\n",
      "2020-12-21 18:03:49 INFO: Loading: tokenize\n",
      "2020-12-21 18:03:49 INFO: Loading: pos\n",
      "2020-12-21 18:03:50 INFO: Loading: lemma\n",
      "2020-12-21 18:03:50 INFO: Done loading processors!\n",
      "2020-12-21 18:03:50 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| pos       | ancora  |\n",
      "| lemma     | ancora  |\n",
      "=======================\n",
      "\n",
      "2020-12-21 18:03:50 INFO: Use device: cpu\n",
      "2020-12-21 18:03:50 INFO: Loading: tokenize\n",
      "2020-12-21 18:03:50 INFO: Loading: pos\n",
      "2020-12-21 18:03:51 INFO: Loading: lemma\n",
      "2020-12-21 18:03:51 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('spanish')\n",
    "nlp_gpu = stanza.Pipeline('es', processors='pos, tokenize, lemma', use_gpu=True)\n",
    "nlp_cpu = stanza.Pipeline('es', processors='pos, tokenize, lemma', use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esg_lemma_gpu = esg[\"sentences\"][:1000].apply(lemmatize, args=[nlp_gpu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esg_lemma_multicore = multiprocess_apply(esg[\"sentences\"][:1000], lemmatize, cores=8, data=[nlp_cpu])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## esg_lemma_multicore = multiprocess_apply(esg[\"sentences\"][:10000], lemmatize, cores=8, data=[nlp_cpu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
